{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/alvaro-francisco-gil/nn4nlp/blob/main/exercises/04_transformers_and_contextuals.ipynb) \n",
        "[![View on GitHub](https://img.shields.io/badge/Open%20on-GitHub-blue?logo=github)](https://github.com/alvaro-francisco-gil/nn4nlp/blob/main/exercises/04_transformers_and_contextuals.ipynb)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "If you are running this notebook in Google Colab, you can install the required packages by running the following cell:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "# !pip install torch transformers pandas numpy seaborn matplotlib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S4ld2bWB62Wm",
        "outputId": "b55083a9-4d72-4d37-d172-835a30e2b6ac"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from torch.optim import AdamW\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "from collections import Counter\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from transformers import (\n",
        "    BertForSequenceClassification,\n",
        "    XLNetForSequenceClassification,\n",
        "    get_linear_schedule_with_warmup,\n",
        "    AutoTokenizer\n",
        ")\n",
        "\n",
        "# Set device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GInQXNpm-p4E"
      },
      "source": [
        "# Analyze Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "g_GhTMQ-6w_W",
        "outputId": "2fc6950b-f7b9-4714-ef56-29d6a79c350c"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>review</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>17527</td>\n",
              "      <td>People tried to make me believe that the premi...</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>24155</td>\n",
              "      <td>I have been wanting to see cut since the day i...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>21972</td>\n",
              "      <td>This movie is terrible. The suspense is spent ...</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4565</td>\n",
              "      <td>I hope she can keep acting and directing. She'...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>22098</td>\n",
              "      <td>I fell in love with this silent action drama. ...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      id                                             review sentiment\n",
              "0  17527  People tried to make me believe that the premi...  negative\n",
              "1  24155  I have been wanting to see cut since the day i...  positive\n",
              "2  21972  This movie is terrible. The suspense is spent ...  negative\n",
              "3   4565  I hope she can keep acting and directing. She'...  positive\n",
              "4  22098  I fell in love with this silent action drama. ...  positive"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_file = '../data/train_reviews.csv'\n",
        "df = pd.read_csv(train_file)\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hV35vmkE7G2I",
        "outputId": "89945ae1-81d3-4e1e-d094-92b62262eb02"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(8000, 3)"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "qUv5lIdV7IfI",
        "outputId": "336080f8-eadc-4474-b6c4-92b90097aae2"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>review</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>6666</td>\n",
              "      <td>Movies about dinosaurs can be entertaining. So...</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>23267</td>\n",
              "      <td>If it's action and adventure you want in a mov...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2897</td>\n",
              "      <td>Barbra Streisand is a tour de force in this Ho...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>23177</td>\n",
              "      <td>I borrowed this movie despite its extremely lo...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>14171</td>\n",
              "      <td>\"Baby Face\" is a precode melodrama starring a ...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      id                                             review sentiment\n",
              "0   6666  Movies about dinosaurs can be entertaining. So...  negative\n",
              "1  23267  If it's action and adventure you want in a mov...  positive\n",
              "2   2897  Barbra Streisand is a tour de force in this Ho...  positive\n",
              "3  23177  I borrowed this movie despite its extremely lo...  positive\n",
              "4  14171  \"Baby Face\" is a precode melodrama starring a ...  positive"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "test_file = '../data/test_reviews.csv'\n",
        "test_df = pd.read_csv(test_file)\n",
        "test_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sjhniwj77O6l",
        "outputId": "3ce0ff65-f23f-4074-f7d1-bc752ff7ac23"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(2000, 3)"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "test_df.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8SmJ2M5Nr7ML"
      },
      "source": [
        "## Check class balance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 178
        },
        "id": "hlJgktQ3sChs",
        "outputId": "f4d7aefb-17da-4480-eb27-55c6796db9b8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "sentiment\n",
              "positive    4012\n",
              "negative    3988\n",
              "Name: count, dtype: int64"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.value_counts('sentiment')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 178
        },
        "id": "Qn_Gnf9TsGDC",
        "outputId": "c0dff444-d590-4493-fea4-85fdd8819b90"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "sentiment\n",
              "negative    1025\n",
              "positive     975\n",
              "Name: count, dtype: int64"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "test_df.value_counts('sentiment')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qtI-rrXGsJ9L"
      },
      "source": [
        "Both classes are balanced"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bucYFby-_L_a"
      },
      "source": [
        "## Check missing values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 178
        },
        "id": "DyFxsqgH-7Xd",
        "outputId": "97615e2f-4433-4423-bebe-f1f919728784"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "id           0\n",
              "review       0\n",
              "sentiment    0\n",
              "dtype: int64"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.isna().sum()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 178
        },
        "id": "l26Hgdj5_AUr",
        "outputId": "899cbdef-3851-4d76-a305-057e685ac702"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "id           0\n",
              "review       0\n",
              "sentiment    0\n",
              "dtype: int64"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "test_df.isna().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-jaT8OEQ_RCY"
      },
      "source": [
        "## Visual Inspection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 91
        },
        "id": "FFokHF407URm",
        "outputId": "937c90e4-2399-4314-a747-8851280f78c3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"I have been wanting to see cut since the day i have heard of it, which was sometime last year. Anyway i got to see today, and when the movie started i thought that it started rather week but it got better after 10 mins or so. I thought that the movie was pretty good. but the thing i didn't like was how the killer was created, i was thinking just before i rented that it would probably suck just like Urban legends: final cut, i almost died it. mostly everything in UL final cut needed to be improved. CUT is 100 times better than UL:final cut. The best part of CUT is the killer and the death scenes. The killer kicks MO F***ING ASS.<br /><br />i give cut a 8 out of 10\""
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.iloc[1]['review']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "id": "AF6OaAAm7nbh",
        "outputId": "83e6d147-2d4b-48f1-c4da-27d6560d285f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'This movie is terrible. The suspense is spent waiting for a point. There isn\\'t much of one.<br /><br />Aside from a few great lines ( \"I found a tooth in my apartment\" ), and the main characters dedication to killing himself, it\\'s a collection of supposedly eerie sounds.<br /><br />'"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.iloc[2]['review']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GNCdiwQ7_jms"
      },
      "source": [
        "Insigths:\n",
        "- Need to remove HTML labels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CRmirT7CAOWs"
      },
      "source": [
        "## Word Histogram"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "aIssCT1zAQhj"
      },
      "outputs": [],
      "source": [
        "def plot_word_histogram(text_array):\n",
        "    # Convert input to a flat list of strings\n",
        "    if isinstance(text_array, str):\n",
        "        # If it's a single string, convert to list\n",
        "        text_array = [text_array]\n",
        "    elif isinstance(text_array, list):\n",
        "        # Flatten nested lists and ensure all elements are strings\n",
        "        flattened = []\n",
        "        for item in text_array:\n",
        "            if isinstance(item, list):\n",
        "                flattened.extend(item)\n",
        "            else:\n",
        "                flattened.append(item)\n",
        "        text_array = [str(item) for item in flattened]\n",
        "    else:\n",
        "        raise ValueError(\"Input must be a string or a list of strings.\")\n",
        "\n",
        "    # Join all text into one string\n",
        "    all_text = ' '.join(text_array)\n",
        "\n",
        "    # Tokenize words by splitting on whitespace\n",
        "    words = all_text.split()\n",
        "\n",
        "    # Count word frequencies\n",
        "    word_counts = Counter(words)\n",
        "    labels, values = zip(*word_counts.items())\n",
        "\n",
        "    # Sort by frequency in descending order\n",
        "    sorted_indices = np.argsort(values)[::-1]\n",
        "    labels = np.array(labels)[sorted_indices]\n",
        "    values = np.array(values)[sorted_indices]\n",
        "\n",
        "    # Plot the histogram for the top 10 most frequent words\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.bar(labels[:10], values[:10], color='skyblue')\n",
        "    plt.xticks(rotation=45, ha='right')\n",
        "    plt.title('Top 10 Most Frequent Words')\n",
        "    plt.xlabel('Words')\n",
        "    plt.ylabel('Frequency')\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 607
        },
        "id": "Ya0LvKbeAkc0",
        "outputId": "d5712a18-42d5-4742-ebee-033ec463e56d"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1000x600 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "plot_word_histogram(df['review'].to_list())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LwkQJ8yVA7ja"
      },
      "source": [
        "# Pre-processing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In this case, minimal preprocessing is required since the BERT tokenizer will handle most of the preprocessing tasks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2-gJGFb3A9h-",
        "outputId": "d4fb090b-9e61-4bfe-e26b-1dff75c50099"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(10000,)"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data = np.concatenate([df['review'].values,test_df['review'].values],axis=0)\n",
        "raw_data = data\n",
        "data.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IZiGdCAfBlnX"
      },
      "source": [
        "## Remove HTML labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "bGNeZtxlBi8g"
      },
      "outputs": [],
      "source": [
        "def clean_htmls_text_array(text_array):\n",
        "\n",
        "    def remove_html_tags(text):\n",
        "        pattern = re.compile('<.*?>')\n",
        "        clean_text = re.sub(pattern, '', text)\n",
        "        clean_text = ' '.join(clean_text.split())\n",
        "        return clean_text\n",
        "\n",
        "    cleaned_array = [remove_html_tags(text) for text in text_array]\n",
        "\n",
        "    return cleaned_array"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "3kCaTiK2B-IM",
        "outputId": "f91af580-4a2e-465c-8604-1f579a3fc12e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'ASS.<br /><br />i give cut a 8 out of 10'"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data[1][-40:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "57XDTN-MBqEI"
      },
      "outputs": [],
      "source": [
        "data = clean_htmls_text_array(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "K-n00afDDMZk",
        "outputId": "caa6cddf-b7be-426d-c620-f486f5554598"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "' MO F***ING ASS.i give cut a 8 out of 10'"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data[1][-40:]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Retain numbers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j0KD3mDtEDHi"
      },
      "source": [
        "I don't want to remove the numbers as they may contain valuable information for the classifier, as in this case: \"8 out of 10\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A5A3s6myFqBO"
      },
      "source": [
        "I test the hypothesis:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "IbamfUVuENxO"
      },
      "outputs": [],
      "source": [
        "def find_term_with_context(text_array, term, context_chars=5):\n",
        "    occurrences = 0\n",
        "    occurrences_list = []\n",
        "\n",
        "    for text in text_array:\n",
        "        start = 0\n",
        "        while (start := text.find(term, start)) != -1:\n",
        "            occurrences += 1\n",
        "            context_start = max(0, start - context_chars)\n",
        "            context_end = min(len(text), start + len(term) + context_chars)\n",
        "            context = text[context_start:context_end]\n",
        "\n",
        "            occurrences_list.append(f\"...{context}...\")\n",
        "            start += len(term)\n",
        "\n",
        "    print(f\"\\nTotal occurrences of '{term}': {occurrences}\")\n",
        "    return occurrences, occurrences_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "WlbHafGeEa4T",
        "outputId": "33378c04-bfdc-45b5-a964-ed2cd1c7bfa1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Total occurrences of 'out of': 2029\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'... a 3 out of 10, ...'"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "_,ocurrences = find_term_with_context(data, 'out of')\n",
        "ocurrences[1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-VC0VWYFt6cx"
      },
      "source": [
        "We see that 2070 reviews contain a possible explicit grade"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e0-M83k18rhP"
      },
      "source": [
        "# Splitting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Take Labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hKUVBHA1B2XU",
        "outputId": "7c433c3a-134a-4826-9ece-39b7b3a9beda"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(10000,)"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "combined_sentiments = np.concatenate([df['sentiment'].values, test_df['sentiment'].values])\n",
        "combined_sentiments.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "label_map = {\"positive\": 1, \"negative\": 0}\n",
        "combined_sentiments = np.array([label_map[label] for label in combined_sentiments])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Data Splitting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_texts, test_texts = data[:8000], data[8000:]\n",
        "train_labels, test_labels = combined_sentiments[:8000], combined_sentiments[8000:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "numpy.int64"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "type(train_texts)\n",
        "type(train_labels[0])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Create Validation Set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
        "    train_texts, train_labels, test_size=0.1, random_state=42\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Modelling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The models we're going to try are BERT and XLNet\n",
        " \n",
        "We're using BERT as a baseline model since it was one of the first transformer models to achieve state-of-the-art results on many NLP tasks.\n",
        "XLNet is an advanced model that outperforms BERT on many benchmarks and is currently ranked first on the NLP Progress leaderboard for sentiment analysis (https://nlpprogress.com/english/sentiment_analysis.html).\n",
        "XLNet improves upon BERT by using a permutation-based training method that overcomes limitations of BERT's masked language modeling approach."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training and Validation Loop\n",
        "Purpose: This core function handles the entire training and validation process for any transformer model.\n",
        "- We need a unified training loop that can work for both BERT and XLNet to ensure fair comparison\n",
        "- The function needs to track both training and validation metrics across epochs\n",
        "- We need to capture training time to compare model efficiency\n",
        "- We use the same hyperparameters (learning rate, optimizer, etc.) for both models for fair comparison"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Function for model training and evaluation\n",
        "def train_evaluate_model(model, train_dataloader, val_dataloader, epochs=10):\n",
        "    optimizer = AdamW(model.parameters(), lr=2e-5, eps=1e-8)\n",
        "    \n",
        "    # Total number of training steps\n",
        "    total_steps = len(train_dataloader) * epochs\n",
        "    \n",
        "    # Create the learning rate scheduler\n",
        "    scheduler = get_linear_schedule_with_warmup(\n",
        "        optimizer, num_warmup_steps=0, num_training_steps=total_steps\n",
        "    )\n",
        "    \n",
        "    # Training loop\n",
        "    training_stats = []\n",
        "    total_training_time = 0\n",
        "    \n",
        "    for epoch in range(epochs):\n",
        "        print(f'\\nEpoch {epoch+1}/{epochs}')\n",
        "        \n",
        "        # Training\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        start_time = time.time()\n",
        "        \n",
        "        for batch in tqdm(train_dataloader, desc=\"Training\"):\n",
        "            batch = tuple(b.to(device) for b in batch)\n",
        "            inputs = {'input_ids': batch[0],\n",
        "                      'attention_mask': batch[1],\n",
        "                      'labels': batch[2]}\n",
        "            \n",
        "            # Clear any previously calculated gradients\n",
        "            model.zero_grad()\n",
        "            \n",
        "            # Forward pass\n",
        "            outputs = model(**inputs)\n",
        "            loss = outputs.loss\n",
        "            total_loss += loss.item()\n",
        "            \n",
        "            # Backward pass to calculate gradients\n",
        "            loss.backward()\n",
        "            \n",
        "            # Clip the norm of the gradients to 1.0 to prevent exploding gradients\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "            \n",
        "            # Update parameters and take a step using the computed gradient\n",
        "            optimizer.step()\n",
        "            \n",
        "            # Update the learning rate\n",
        "            scheduler.step()\n",
        "        \n",
        "        epoch_time = time.time() - start_time\n",
        "        total_training_time += epoch_time\n",
        "        avg_train_loss = total_loss / len(train_dataloader)\n",
        "        \n",
        "        print(f\"Training Loss: {avg_train_loss:.4f}\")\n",
        "        print(f\"Training epoch took: {epoch_time:.4f} seconds\")\n",
        "        \n",
        "        # Validation\n",
        "        model.eval()\n",
        "        val_loss = 0\n",
        "        val_preds = []\n",
        "        val_true = []\n",
        "        \n",
        "        for batch in tqdm(val_dataloader, desc=\"Validation\"):\n",
        "            batch = tuple(b.to(device) for b in batch)\n",
        "            inputs = {'input_ids': batch[0],\n",
        "                      'attention_mask': batch[1],\n",
        "                      'labels': batch[2]}\n",
        "            \n",
        "            with torch.no_grad():\n",
        "                outputs = model(**inputs)\n",
        "            \n",
        "            loss = outputs.loss\n",
        "            val_loss += loss.item()\n",
        "            \n",
        "            logits = outputs.logits\n",
        "            preds = torch.argmax(logits, dim=1).flatten()\n",
        "            val_preds.extend(preds.cpu().numpy())\n",
        "            val_true.extend(inputs['labels'].cpu().numpy())\n",
        "        \n",
        "        avg_val_loss = val_loss / len(val_dataloader)\n",
        "        val_accuracy = accuracy_score(val_true, val_preds)\n",
        "        \n",
        "        print(f\"Validation Loss: {avg_val_loss:.4f}\")\n",
        "        print(f\"Validation Accuracy: {val_accuracy:.4f}\")\n",
        "        \n",
        "        training_stats.append({\n",
        "            'epoch': epoch + 1,\n",
        "            'train_loss': avg_train_loss,\n",
        "            'val_loss': avg_val_loss,\n",
        "            'val_accuracy': val_accuracy,\n",
        "            'epoch_time': epoch_time,\n",
        "        })\n",
        "    \n",
        "    print(f\"\\nTotal training time: {total_training_time:.4f} seconds\")\n",
        "    return training_stats"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Evaluation Function\n",
        "Purpose: Provides standardized evaluation of model performance on unseen test data.\n",
        "- We need a consistent way to evaluate both models on the same test set\n",
        "- The function should return both the overall accuracy and detailed prediction information\n",
        "- Classification reports give a more nuanced understanding of performance across classes\n",
        "- This separated evaluation function ensures both models are tested in exactly the same way"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate_model(model, test_dataloader):\n",
        "    model.eval()\n",
        "    test_preds = []\n",
        "    test_true = []\n",
        "    \n",
        "    for batch in tqdm(test_dataloader, desc=\"Testing\"):\n",
        "        batch = tuple(b.to(device) for b in batch)\n",
        "        inputs = {'input_ids': batch[0],\n",
        "                  'attention_mask': batch[1],\n",
        "                  'labels': batch[2]}\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            outputs = model(**inputs)\n",
        "        \n",
        "        logits = outputs.logits\n",
        "        preds = torch.argmax(logits, dim=1).flatten()\n",
        "        test_preds.extend(preds.cpu().numpy())\n",
        "        test_true.extend(inputs['labels'].cpu().numpy())\n",
        "    \n",
        "    test_accuracy = accuracy_score(test_true, test_preds)\n",
        "    print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
        "    print(\"\\nClassification Report:\")\n",
        "    print(classification_report(test_true, test_preds, target_names=['negative', 'positive']))\n",
        "    \n",
        "    return test_accuracy, test_preds"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Plot Comparison\n",
        "Purpose: Provides standardized evaluation of model performance on unseen test data.\n",
        "- We need a consistent way to evaluate both models on the same test set\n",
        "- The function should return both the overall accuracy and detailed prediction information\n",
        "- Classification reports give a more nuanced understanding of performance across classes\n",
        "- This separated evaluation function ensures both models are tested in exactly the same way"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_comparison(bert_stats, xlnet_stats):\n",
        "    \"\"\"Plot comparison between BERT and XLNet training metrics\"\"\"\n",
        "    epochs = range(1, len(bert_stats) + 1)\n",
        "    \n",
        "    plt.figure(figsize=(15, 5))\n",
        "    \n",
        "    # Training loss\n",
        "    plt.subplot(1, 3, 1)\n",
        "    plt.plot([s['train_loss'] for s in bert_stats], 'b-o', label='BERT')\n",
        "    plt.plot([s['train_loss'] for s in xlnet_stats], 'r-o', label='XLNet')\n",
        "    plt.title('Training Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    \n",
        "    # Validation loss\n",
        "    plt.subplot(1, 3, 2)\n",
        "    plt.plot([s['val_loss'] for s in bert_stats], 'b-o', label='BERT')\n",
        "    plt.plot([s['val_loss'] for s in xlnet_stats], 'r-o', label='XLNet')\n",
        "    plt.title('Validation Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    \n",
        "    # Validation accuracy\n",
        "    plt.subplot(1, 3, 3)\n",
        "    plt.plot([s['val_accuracy'] for s in bert_stats], 'b-o', label='BERT')\n",
        "    plt.plot([s['val_accuracy'] for s in xlnet_stats], 'r-o', label='XLNet')\n",
        "    plt.title('Validation Accuracy')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.savefig('bert_xlnet_comparison.png')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## BERT Model Implementation\n",
        "Purpose: Encapsulates the complete BERT implementation workflow.\n",
        "- We need to isolate BERT-specific processing from the general training loop\n",
        "- BERT requires specific tokenization that differs from XLNet:\n",
        "  - Uses an uncased vocabulary (lowercase)\n",
        "  - Has its own tokenization rules and special tokens\n",
        "- We need sequential steps: tokenize → create datasets → create data loaders → initialize model → train → evaluate\n",
        "- Encapsulating this in a function keeps our code organized and modular"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [],
      "source": [
        "def run_bert(train_texts, val_texts, test_texts, train_labels, val_labels, test_labels):\n",
        "    print(\"\\n======= Running BERT Model =======\\n\")\n",
        "    # Tokenizing with BERT\n",
        "    bert_tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
        "    \n",
        "    def tokenize_bert(texts):\n",
        "        return bert_tokenizer(\n",
        "            texts, \n",
        "            truncation=True, \n",
        "            padding='max_length', \n",
        "            max_length=128,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "    \n",
        "    # Tokenize datasets\n",
        "    train_encodings = tokenize_bert(train_texts)\n",
        "    val_encodings = tokenize_bert(val_texts)\n",
        "    test_encodings = tokenize_bert(test_texts)\n",
        "    \n",
        "    # Create dataset objects\n",
        "    train_dataset = TensorDataset(\n",
        "        train_encodings['input_ids'],\n",
        "        train_encodings['attention_mask'],\n",
        "        torch.tensor(train_labels)\n",
        "    )\n",
        "    \n",
        "    val_dataset = TensorDataset(\n",
        "        val_encodings['input_ids'],\n",
        "        val_encodings['attention_mask'],\n",
        "        torch.tensor(val_labels)\n",
        "    )\n",
        "    \n",
        "    test_dataset = TensorDataset(\n",
        "        test_encodings['input_ids'],\n",
        "        test_encodings['attention_mask'],\n",
        "        torch.tensor(test_labels)\n",
        "    )\n",
        "    \n",
        "    # Create dataloaders\n",
        "    train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "    val_dataloader = DataLoader(val_dataset, batch_size=32)\n",
        "    test_dataloader = DataLoader(test_dataset, batch_size=32)\n",
        "    \n",
        "    # Load BERT model\n",
        "    bert_model = BertForSequenceClassification.from_pretrained(\n",
        "        'bert-base-uncased',\n",
        "        num_labels=2\n",
        "    ).to(device)\n",
        "    \n",
        "    # Train and evaluate\n",
        "    bert_stats = train_evaluate_model(bert_model, train_dataloader, val_dataloader)\n",
        "    bert_accuracy, bert_preds = evaluate_model(bert_model, test_dataloader)\n",
        "    \n",
        "    return bert_accuracy, bert_stats, bert_model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## XLNet Model Implementation\n",
        "Purpose: Encapsulates the complete XLNet implementation workflow.\n",
        "- Similar structure to the BERT function, but with XLNet-specific details\n",
        "- XLNet requires different tokenization:\n",
        "  - Uses a cased vocabulary (preserves case)\n",
        "  - Has different special tokens and formatting requirements\n",
        "- Using a parallel function structure to run_bert makes the comparison clean and fair\n",
        "- Maintaining consistency in the workflow ensures differences are due to the models, not implementation details"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [],
      "source": [
        "# XLNet model implementation\n",
        "def run_xlnet(train_texts, val_texts, test_texts, train_labels, val_labels, test_labels):\n",
        "    print(\"\\n======= Running XLNet Model =======\\n\")\n",
        "    # Tokenizing with XLNet\n",
        "    xlnet_tokenizer = AutoTokenizer.from_pretrained('xlnet-base-cased')\n",
        "    \n",
        "    # XLNet has different formatting requirements\n",
        "    def tokenize_xlnet(texts):\n",
        "        return xlnet_tokenizer(\n",
        "            texts, \n",
        "            truncation=True, \n",
        "            padding='max_length', \n",
        "            max_length=128,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "    \n",
        "    # Tokenize datasets\n",
        "    train_encodings = tokenize_xlnet(train_texts)\n",
        "    val_encodings = tokenize_xlnet(val_texts)\n",
        "    test_encodings = tokenize_xlnet(test_texts)\n",
        "    \n",
        "    # Create dataset objects\n",
        "    train_dataset = TensorDataset(\n",
        "        train_encodings['input_ids'],\n",
        "        train_encodings['attention_mask'],\n",
        "        torch.tensor(train_labels)\n",
        "    )\n",
        "    \n",
        "    val_dataset = TensorDataset(\n",
        "        val_encodings['input_ids'],\n",
        "        val_encodings['attention_mask'],\n",
        "        torch.tensor(val_labels)\n",
        "    )\n",
        "    \n",
        "    test_dataset = TensorDataset(\n",
        "        test_encodings['input_ids'],\n",
        "        test_encodings['attention_mask'],\n",
        "        torch.tensor(test_labels)\n",
        "    )\n",
        "    \n",
        "    # Create dataloaders\n",
        "    train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "    val_dataloader = DataLoader(val_dataset, batch_size=32)\n",
        "    test_dataloader = DataLoader(test_dataset, batch_size=32)\n",
        "    \n",
        "    # Load XLNet model\n",
        "    xlnet_model = XLNetForSequenceClassification.from_pretrained(\n",
        "        'xlnet-base-cased',\n",
        "        num_labels=2\n",
        "    ).to(device)\n",
        "    \n",
        "    # Train and evaluate\n",
        "    xlnet_stats = train_evaluate_model(xlnet_model, train_dataloader, val_dataloader)\n",
        "    xlnet_accuracy, xlnet_preds = evaluate_model(xlnet_model, test_dataloader)\n",
        "    \n",
        "    return xlnet_accuracy, xlnet_stats, xlnet_model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Time Comparison Analysis\n",
        "Purpose: Provides a comprehensive comparison of training times between BERT and XLNet.\n",
        "- We need to track both total training time and per-epoch timing\n",
        "- This helps us understand the performance vs. time trade-off for each model\n",
        "- We can also calculate speedup/slowdown percentages to highlight relative efficiency\n",
        "- This analysis is crucial for understanding the practical performance of each model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Time comparison analysis\n",
        "def create_time_comparison_plots(bert_stats, xlnet_stats):\n",
        "    # Extract timing data\n",
        "    bert_times = [stat['epoch_time'] for stat in bert_stats]\n",
        "    xlnet_times = [stat['epoch_time'] for stat in xlnet_stats]\n",
        "    \n",
        "    # Calculate total times\n",
        "    bert_total = sum(bert_times)\n",
        "    xlnet_total = sum(xlnet_times)\n",
        "    \n",
        "    # Create time comparison bar chart\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    \n",
        "    # Epoch-wise timing comparison\n",
        "    plt.subplot(1, 2, 1)\n",
        "    epochs = range(1, len(bert_times) + 1)\n",
        "    width = 0.35\n",
        "    \n",
        "    plt.bar([x - width/2 for x in epochs], bert_times, width, label='BERT', color='royalblue')\n",
        "    plt.bar([x + width/2 for x in epochs], xlnet_times, width, label='XLNet', color='firebrick')\n",
        "    \n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Time (seconds)')\n",
        "    plt.title('Training Time per Epoch')\n",
        "    plt.xticks(epochs)\n",
        "    plt.legend()\n",
        "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "    \n",
        "    # Total time comparison\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.bar(['BERT', 'XLNet'], [bert_total, xlnet_total], color=['royalblue', 'firebrick'])\n",
        "    plt.ylabel('Time (seconds)')\n",
        "    plt.title('Total Training Time')\n",
        "    \n",
        "    # Add values on top of the bars\n",
        "    for i, v in enumerate([bert_total, xlnet_total]):\n",
        "        plt.text(i, v + 5, f\"{v:.1f}s\", ha='center')\n",
        "    \n",
        "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "    \n",
        "    # Calculate the speedup/slowdown\n",
        "    speedup = (xlnet_total - bert_total) / xlnet_total * 100 if xlnet_total > bert_total else (bert_total - xlnet_total) / bert_total * 100\n",
        "    faster_model = \"BERT\" if bert_total < xlnet_total else \"XLNet\"\n",
        "    \n",
        "    plt.figtext(0.5, 0.01, f\"{faster_model} was {speedup:.1f}% faster\", ha=\"center\", fontsize=12, bbox={\"facecolor\":\"yellow\", \"alpha\":0.5})\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    # Return timing info for further analysis\n",
        "    return {\n",
        "        'bert_times': bert_times,\n",
        "        'xlnet_times': xlnet_times,\n",
        "        'bert_total': bert_total,\n",
        "        'xlnet_total': xlnet_total,\n",
        "        'faster_model': faster_model,\n",
        "        'speedup_percentage': speedup\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training Time vs. Performance Analysis\n",
        "Purpose: Visualizes the relationship between training time and model performance.\n",
        "- We need to understand how training time affects model accuracy\n",
        "- This helps us identify the trade-off between training speed and model quality\n",
        "- It also allows us to compare the convergence rates of different models\n",
        "- This analysis is useful for understanding the practical performance of each model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_time_vs_performance(bert_stats, xlnet_stats, bert_accuracy, xlnet_accuracy):\n",
        "    # Calculate cumulative training time at each epoch\n",
        "    bert_cumulative_times = []\n",
        "    xlnet_cumulative_times = []\n",
        "    \n",
        "    bert_time = 0\n",
        "    xlnet_time = 0\n",
        "    \n",
        "    for i in range(len(bert_stats)):\n",
        "        bert_time += bert_stats[i]['epoch_time']\n",
        "        bert_cumulative_times.append(bert_time)\n",
        "        \n",
        "        if i < len(xlnet_stats):  # In case the number of epochs differ\n",
        "            xlnet_time += xlnet_stats[i]['epoch_time']\n",
        "            xlnet_cumulative_times.append(xlnet_time)\n",
        "    \n",
        "    # Plot accuracy vs. cumulative training time\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    \n",
        "    # Plot validation accuracy vs. cumulative time\n",
        "    plt.plot(bert_cumulative_times, [s['val_accuracy'] for s in bert_stats], 'bo-', label='BERT Val Accuracy')\n",
        "    plt.plot(xlnet_cumulative_times, [s['val_accuracy'] for s in xlnet_stats], 'ro-', label='XLNet Val Accuracy')\n",
        "    \n",
        "    # Add test accuracy as endpoints\n",
        "    plt.plot(bert_cumulative_times[-1], bert_accuracy, 'bs', markersize=10, label='BERT Test Accuracy')\n",
        "    plt.plot(xlnet_cumulative_times[-1], xlnet_accuracy, 'rs', markersize=10, label='XLNet Test Accuracy')\n",
        "    \n",
        "    plt.xlabel('Cumulative Training Time (seconds)')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.title('Accuracy vs. Training Time')\n",
        "    plt.grid(True, linestyle='--', alpha=0.7)\n",
        "    plt.legend()\n",
        "    \n",
        "    # Add annotations\n",
        "    for i, (time, acc) in enumerate(zip(bert_cumulative_times, [s['val_accuracy'] for s in bert_stats])):\n",
        "        plt.annotate(f\"Epoch {i+1}\", (time, acc), textcoords=\"offset points\", xytext=(0,10), ha='center')\n",
        "        \n",
        "    for i, (time, acc) in enumerate(zip(xlnet_cumulative_times, [s['val_accuracy'] for s in xlnet_stats])):\n",
        "        plt.annotate(f\"Epoch {i+1}\", (time, acc), textcoords=\"offset points\", xytext=(0,-15), ha='center')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Executing the models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "======= Running BERT Model =======\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 1/10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training:  13%|█▎        | 59/450 [00:05<00:35, 11.13it/s]"
          ]
        }
      ],
      "source": [
        "SAMPLE_SIZE = 1000\n",
        "\n",
        "# Run BERT model\n",
        "bert_accuracy, bert_stats, bert_model = run_bert(train_texts, val_texts, test_texts, train_labels, val_labels, test_labels)\n",
        "\n",
        "# Run XLNet model\n",
        "xlnet_accuracy, xlnet_stats, xlnet_model = run_xlnet(train_texts, val_texts, test_texts, train_labels, val_labels, test_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compare results\n",
        "print(\"\\n======= Model Comparison =======\")\n",
        "print(f\"BERT Accuracy: {bert_accuracy:.4f}\")\n",
        "print(f\"XLNet Accuracy: {xlnet_accuracy:.4f}\")\n",
        "\n",
        "if bert_accuracy > xlnet_accuracy:\n",
        "    print(\"BERT outperformed XLNet on this dataset\")\n",
        "elif xlnet_accuracy > bert_accuracy:\n",
        "    print(\"XLNet outperformed BERT on this dataset\")\n",
        "else:\n",
        "    print(\"Both models performed equally well\")\n",
        "\n",
        "print(\"\\nTraining Statistics:\")\n",
        "print(\"BERT:\")\n",
        "for stat in bert_stats:\n",
        "    print(f\"Epoch {stat['epoch']}: Train Loss = {stat['train_loss']:.4f}, Val Loss = {stat['val_loss']:.4f}, Val Accuracy = {stat['val_accuracy']:.4f}\")\n",
        "\n",
        "print(\"\\nXLNet:\")\n",
        "for stat in xlnet_stats:\n",
        "    print(f\"Epoch {stat['epoch']}: Train Loss = {stat['train_loss']:.4f}, Val Loss = {stat['val_loss']:.4f}, Val Accuracy = {stat['val_accuracy']:.4f}\")\n",
        "\n",
        "# Plot comparison\n",
        "plot_comparison(bert_stats, xlnet_stats)\n",
        "\n",
        "# Calculate and display total training times\n",
        "print(\"\\nKey findings:\")\n",
        "print(\"1. Training time comparison:\")\n",
        "bert_time = sum(stat['epoch_time'] for stat in bert_stats)\n",
        "xlnet_time = sum(stat['epoch_time'] for stat in xlnet_stats)\n",
        "print(f\"   - BERT total training time: {bert_time:.2f} seconds\")\n",
        "print(f\"   - XLNet total training time: {xlnet_time:.2f} seconds\")\n",
        "\n",
        "print(\"\\n2. Final performance:\")\n",
        "print(f\"   - BERT test accuracy: {bert_accuracy:.4f}\")\n",
        "print(f\"   - XLNet test accuracy: {xlnet_accuracy:.4f}\")\n",
        "\n",
        "print(\"\\n3. Learning curve:\")\n",
        "bert_val_accuracies = \", \".join([f\"{stat['val_accuracy']:.4f}\" for stat in bert_stats])\n",
        "xlnet_val_accuracies = \", \".join([f\"{stat['val_accuracy']:.4f}\" for stat in xlnet_stats])\n",
        "print(f\"   - BERT validation accuracy evolution: [{bert_val_accuracies}]\")\n",
        "print(f\"   - XLNet validation accuracy evolution: [{xlnet_val_accuracies}]\")\n",
        "\n",
        "# Run the time comparison analyses after training both models\n",
        "time_comparison = create_time_comparison_plots(bert_stats, xlnet_stats)\n",
        "plot_time_vs_performance(bert_stats, xlnet_stats, bert_accuracy, xlnet_accuracy)\n",
        "\n",
        "# Print detailed time analysis\n",
        "print(\"\\n=== Detailed Time Analysis ===\")\n",
        "print(f\"BERT average epoch time: {sum(time_comparison['bert_times'])/len(time_comparison['bert_times']):.2f} seconds\")\n",
        "print(f\"XLNet average epoch time: {sum(time_comparison['xlnet_times'])/len(time_comparison['xlnet_times']):.2f} seconds\")\n",
        "\n",
        "# Time per sample analysis\n",
        "num_train_samples = len(train_texts)\n",
        "print(f\"\\nTraining time per sample:\")\n",
        "print(f\"BERT: {time_comparison['bert_total']/num_train_samples*1000:.2f} milliseconds/sample\")\n",
        "print(f\"XLNet: {time_comparison['xlnet_total']/num_train_samples*1000:.2f} milliseconds/sample\")\n",
        "\n",
        "# Performance vs time trade-off\n",
        "print(\"\\nEfficiency analysis (accuracy per second of training):\")\n",
        "bert_efficiency = bert_accuracy / time_comparison['bert_total'] * 100\n",
        "xlnet_efficiency = xlnet_accuracy / time_comparison['xlnet_total'] * 100\n",
        "print(f\"BERT: {bert_efficiency:.4f}% accuracy per second of training\")\n",
        "print(f\"XLNet: {xlnet_efficiency:.4f}% accuracy per second of training\")\n",
        "\n",
        "more_efficient = \"BERT\" if bert_efficiency > xlnet_efficiency else \"XLNet\"\n",
        "efficiency_ratio = bert_efficiency/xlnet_efficiency if bert_efficiency > xlnet_efficiency else xlnet_efficiency/bert_efficiency\n",
        "print(f\"\\n{more_efficient} is {efficiency_ratio:.2f}x more efficient in terms of accuracy per training time.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Thanks for reading the code, connect with me on [LinkedIn](https://www.linkedin.com/in/alvaro-francisco-gil/) or [GitHub](https://github.com/alvaro-francisco-gil) if you have any questions or comments.\n",
        "\n",
        "*Álvaro Francisco Gil*"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
